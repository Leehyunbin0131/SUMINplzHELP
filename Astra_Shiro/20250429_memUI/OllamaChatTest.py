import requests
import json
import time
import logging
import threading
import argparse
import collections 
import os
from logging.handlers import RotatingFileHandler

try:
    import config # ì„¤ì • íŒŒì¼ ì„í¬íŠ¸
except ModuleNotFoundError:
    print("ì˜¤ë¥˜: config.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OllamaChatTest.pyì™€ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit() # config íŒŒì¼ ì—†ìœ¼ë©´ ì‹¤í–‰ ì¤‘ì§€

try:
    from mem0 import Memory # mem0 ì„í¬íŠ¸
except ModuleNotFoundError:
    print("ì˜¤ë¥˜: mem0 ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'pip install mem0-py'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    exit()

try:
    # system_promptsì—ì„œ í•„ìš”í•œ ìš”ì†Œë“¤ì„ ëª…ì‹œì ìœ¼ë¡œ ì„í¬íŠ¸
    from system_prompts import (
        # MEMORY_PROMPTS, # ì¤‘ìš”ë„ í‰ê°€ í”„ë¡¬í”„íŠ¸ ë” ì´ìƒ ì‚¬ìš© ì•ˆ í•¨
        MAIN_PROMPT_TEMPLATE,
        get_astra_siro_identity_context # ìƒˆë¡œ ì¶”ê°€ëœ í•¨ìˆ˜ ì„í¬íŠ¸
    )
except ModuleNotFoundError:
    print("ì˜¤ë¥˜: system_prompts.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OllamaChatTest.pyì™€ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit()

# --- ì„ íƒì  ì„í¬íŠ¸ (ìŒì„± ì…ë ¥ìš©) ---
try:
    from RealtimeSTT import AudioToTextRecorder
    REALTIME_STT_AVAILABLE = True
except ModuleNotFoundError:
    REALTIME_STT_AVAILABLE = False
    # ëŒ€ì²´ í´ë˜ìŠ¤ ì •ì˜ (ìŒì„± ì…ë ¥ ë¶ˆê°€ ì‹œ ì‚¬ìš©)
    class AudioToTextRecorder:
        def __init__(self, *args, **kwargs):
            # ë¡œê±°ê°€ ì•„ì§ ì„¤ì •ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ print ì‚¬ìš©
            print("[ê²½ê³ ] RealtimeSTT ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ í…ìŠ¤íŠ¸ ì…ë ¥ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            pass
        def text(self):
            try:
                # ì‚¬ìš©ìì—ê²Œ ëª…í™•íˆ í…ìŠ¤íŠ¸ ì…ë ¥ì„ì„ ì•Œë¦¼
                return input("[í…ìŠ¤íŠ¸ ì…ë ¥]: ")
            except EOFError:
                 return None # ë¹„ëŒ€í™”í˜• í™˜ê²½ì—ì„œ ì¢…ë£Œ ì²˜ë¦¬
        def shutdown(self):
            pass
# --- ì„í¬íŠ¸ ë ---

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
LOG_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'logs')
os.makedirs(LOG_DIR, exist_ok=True)

# ëª¨ë“ˆë³„ ë¡œê±° ì„¤ì • í•¨ìˆ˜
def setup_module_logger(name, log_file, level=logging.INFO):
    formatter = logging.Formatter(config.LOG_FORMAT)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì •
    file_handler = RotatingFileHandler(
        os.path.join(LOG_DIR, log_file),
        maxBytes=5*1024*1024,  # 5MB
        backupCount=3,
        encoding='utf-8'  # UTF-8 ì¸ì½”ë”© ëª…ì‹œì  ì„¤ì •
    )
    file_handler.setFormatter(formatter)
    file_handler.setLevel(level)
    
    # ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ëŠ” í•„ìš” ì—†ìŒ (ëª¨ë“ˆë³„ ì¶œë ¥ ë¶„ë¦¬ ëª©ì )
    
    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.addHandler(file_handler)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
    for hdlr in logger.handlers[:]:
        if isinstance(hdlr, logging.StreamHandler) and not isinstance(hdlr, logging.FileHandler):
            logger.removeHandler(hdlr)
    
    return logger

# ë©”ì¸ ë¡œê±° (ê¸°ë³¸ ì„¤ì •)
logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL, 'INFO'),
    format=config.LOG_FORMAT
)

# ëª¨ë“ˆë³„ ë¡œê±° ì„¤ì •
main_logger = setup_module_logger('main', 'main.log')
stt_logger = setup_module_logger('stt', 'stt.log')
ltm_logger = setup_module_logger('ltm', 'ltm.log')
stm_logger = setup_module_logger('stm', 'stm.log')
llm_logger = setup_module_logger('llm', 'llm.log')

if not REALTIME_STT_AVAILABLE:
     # ë¡œê±° ì„¤ì • í›„ ê²½ê³  ë©”ì‹œì§€ ë¡œê¹…
     stt_logger.warning("RealtimeSTT ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œë§Œ ë™ì‘í•©ë‹ˆë‹¤.")

class VoiceLLMAssistant:
    def __init__(self, ollama_host=config.OLLAMA_HOST, model=config.DEFAULT_MODEL,
                 temperature=config.TEMPERATURE, stt_model=config.STT_MODEL, use_cuda=config.USE_CUDA):
        """
        STT ë° ìƒˆë¡œìš´ LTM/STM ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ ê°–ì¶˜ ìŒì„± LLM ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        self.ollama_url = f"http://{ollama_host}:{config.OLLAMA_PORT}/api/generate"
        self.model = model
        self.temperature = temperature
        self.stt_model = stt_model
        # RealtimeSTT ì—†ìœ¼ë©´ use_cuda, compute_type ì˜ë¯¸ ì—†ì„ ìˆ˜ ìˆìŒ
        self.device = "cuda" if use_cuda and REALTIME_STT_AVAILABLE else "cpu"
        self.compute_type = config.COMPUTE_TYPE if use_cuda and REALTIME_STT_AVAILABLE else "default"
        self.is_processing = False
        self.processing_lock = threading.Lock()

        self.test_ollama_connection(self.ollama_url.replace('/api/generate', '/api/version'), "ë©”ì¸ LLM")
        self.long_term_memory = self.setup_mem0_for_ltm()
        self.short_term_memory = collections.deque(maxlen=10)
        main_logger.info("ë‹¨ê¸° ê¸°ì–µ ë²„í¼ (ìµœëŒ€ 10í„´) ì´ˆê¸°í™” ì™„ë£Œ")

        try:
            self.setup_stt_recorder()
        except Exception as e:
            stt_logger.error(f"STT ë ˆì½”ë” ì„¤ì • ì‹¤íŒ¨: {e}")
            if REALTIME_STT_AVAILABLE:
                raise RuntimeError("ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    def setup_mem0_for_ltm(self):
        """LTM ì €ì¥ì„ ìœ„í•œ mem0 Memory ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
        ltm_logger.info("LTM ì €ì¥ìš© Memory ì‹œìŠ¤í…œ ì„¤ì • ì¤‘ (ChromaDB ì‚¬ìš©)...")
        mem0_config = {
            "vector_store": {
                "provider": config.VECTOR_STORE_PROVIDER,
                "config": {
                    "collection_name": config.CHROMA_COLLECTION,
                    "path": config.CHROMA_PATH,
                },
            },
            "llm": { # ë¹„ë¡ ì¤‘ìš”ë„ í‰ê°€ëŠ” ì•ˆí•˜ì§€ë§Œ, mem0 ë‚´ë¶€ ë‹¤ë¥¸ ìš©ë„ë¡œ ì“¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìœ ì§€
                "provider": "ollama",
                "config": {
                    "model": config.MEM0_LLM_MODEL,
                    "ollama_base_url": config.MEM0_OLLAMA_BASE_URL,
                },
            },
            "embedder": { # LTM ì„ë² ë”©ì— í•„ìˆ˜
                "provider": "ollama",
                "config": {
                    "model": config.MEM0_EMBEDDING_MODEL,
                    "ollama_base_url": config.MEM0_OLLAMA_BASE_URL,
                },
            },
        }
        try:
            self.test_ollama_connection(f"{config.MEM0_OLLAMA_BASE_URL}/api/version", "ë©”ëª¨ë¦¬ LLM/ì„ë² ë”")
            memory_instance = Memory.from_config(mem0_config)
            ltm_logger.info(f"LTM ì €ì¥ìš© Memory ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ (Vector Store: ChromaDB at '{config.CHROMA_PATH}', Embedder: Ollama)")
            return memory_instance
        except Exception as e:
            ltm_logger.error(f"LTM ì €ì¥ìš© Memory ì‹œìŠ¤í…œ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"LTM Memory ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

    def test_ollama_connection(self, server_url, server_name="Ollama"):
        """ì§€ì •ëœ Ollama ì„œë²„ URLì— ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤."""
        try:
            response = requests.get(server_url, timeout=config.REQUEST_TIMEOUT)
            response.raise_for_status()
            if "LLM" in server_name:
                llm_logger.info(f"{server_name} ì„œë²„ì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë¨ ({server_url}): {response.json()}")
            else:
                ltm_logger.info(f"{server_name} ì„œë²„ì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë¨ ({server_url}): {response.json()}")
        except requests.exceptions.RequestException as e:
            if "LLM" in server_name:
                llm_logger.error(f"{server_url}ì— ìˆëŠ” {server_name} ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŒ: {e}")
            else:
                ltm_logger.error(f"{server_url}ì— ìˆëŠ” {server_name} ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŒ: {e}")
            raise ConnectionError(f"{server_name} ì„œë²„ ì—°ê²° ì‹¤íŒ¨. {server_url}ì—ì„œ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")

    def setup_stt_recorder(self):
        """RealtimeSTT ë ˆì½”ë” ë˜ëŠ” ëŒ€ì²´ í…ìŠ¤íŠ¸ ì…ë ¥ê¸°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
        if REALTIME_STT_AVAILABLE:
            stt_logger.info("RealtimeSTT ë ˆì½”ë” ì„¤ì • ì¤‘...")
            recorder_config = {
                "model": self.stt_model,
                "language": 'ko',
                "device": self.device,
                "compute_type": self.compute_type,
                "on_recording_start": self._on_recording_start,
                "on_recording_stop": self._on_recording_stop,
                # --- ìˆ˜ì •ëœ ë¶€ë¶„ ---
                # ëŒë‹¤ í•¨ìˆ˜ê°€ ì¸ì í•˜ë‚˜(audio_data)ë¥¼ ë°›ë„ë¡ ìˆ˜ì •
                "on_transcription_start": lambda audio_data: stt_logger.info("ì˜¤ë””ì˜¤ ë³€í™˜ ì¤‘..."),
                # --- ìˆ˜ì • ë ---
                "enable_realtime_transcription": True,
                "on_realtime_transcription_update": self._on_realtime_update,
                "silero_sensitivity": config.SILERO_SENSITIVITY,
                "min_length_of_recording": config.MIN_RECORDING_LENGTH,
                "post_speech_silence_duration": config.POST_SPEECH_SILENCE,
                "early_transcription_on_silence": config.EARLY_TRANSCRIPTION_SILENCE,
                "spinner": config.SHOW_SPINNER,
                "print_transcription_time": config.PRINT_TRANSCRIPTION_TIME,
                "debug_mode": config.DEBUG_MODE
            }
            self.recorder = AudioToTextRecorder(**recorder_config)
            stt_logger.info("RealtimeSTT ë ˆì½”ë” ì„¤ì • ì™„ë£Œ")
        else:
            stt_logger.info("í…ìŠ¤íŠ¸ ì…ë ¥ ëª¨ë“œë¡œ ë ˆì½”ë” ì„¤ì • (RealtimeSTT ì—†ìŒ)")
            self.recorder = AudioToTextRecorder()

    def _on_recording_start(self): stt_logger.info("ğŸ¤ ë…¹ìŒ ì‹œì‘ë¨")
    def _on_recording_stop(self): stt_logger.info("ğŸ›‘ ë…¹ìŒ ì¤‘ì§€ë¨, ë³€í™˜ ì²˜ë¦¬ ì¤‘...")
    def _on_realtime_update(self, text): print(f"\rğŸ¤ {text}", end="", flush=True)

    def save_to_ltm(self, conversation_text):
        """
        **ìˆ˜ì •ë¨:** ì¤‘ìš”ë„ í‰ê°€ ì—†ì´ ëª¨ë“  ëŒ€í™” ë‚´ìš©ì„ LTMì— ì €ì¥í•©ë‹ˆë‹¤.
        ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        thread_id = threading.get_ident()
        ltm_logger.info(f"LTM ì €ì¥ ì§„í–‰ ì¤‘... (ìŠ¤ë ˆë“œ ID: {thread_id})")
        try:
            self.long_term_memory.add(
                conversation_text,
                user_id=config.MEMORY_USER_ID,
            )
            ltm_logger.info(f"ëŒ€í™” ë‚´ìš©ì„ LTMì— ì €ì¥í–ˆìŠµë‹ˆë‹¤: {conversation_text[:100]}... (ìŠ¤ë ˆë“œ ID: {thread_id})")
        except Exception as e:
            ltm_logger.error(f"LTM ì €ì¥ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (ìŠ¤ë ˆë“œ ID: {thread_id}): {e}", exc_info=True)


    def process_voice_input(self):
        """ìŒì„± ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ì„ ì²˜ë¦¬í•˜ê³ , ê²°ê³¼ë¥¼ ì–»ì–´ ë©”ì¸ LLMì— ì „ì†¡í•©ë‹ˆë‹¤."""
        if self.is_processing:
            main_logger.warning("ì´ë¯¸ ìš”ì²­ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”")
            return
        with self.processing_lock:
            self.is_processing = True
            transcribed_text = None
            try:
                if REALTIME_STT_AVAILABLE:
                    print("\nâ³ ì§ˆë¬¸ì„ ë“£ê³  ìˆìŠµë‹ˆë‹¤... (ì§€ê¸ˆ ë§ì”€í•˜ì„¸ìš”)")
                transcribed_text = self.recorder.text()
                if not transcribed_text or transcribed_text.strip() == "":
                    if transcribed_text is None:
                         raise KeyboardInterrupt("ì…ë ¥ ì¢…ë£Œë¨")
                    print("\nì…ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
                    self.is_processing = False
                    return
                input_source = "ğŸ¤ ë§ì”€í•˜ì‹  ë‚´ìš©" if REALTIME_STT_AVAILABLE else "âŒ¨ï¸  ì…ë ¥í•˜ì‹  ë‚´ìš©"
                print(f"\n\n{input_source}: {transcribed_text}")
                print("\nâ³ LLM ìƒê° ì¤‘ (ì •ì²´ì„± + STM + LTM ì‚¬ìš©)...")
                self.send_to_llm(transcribed_text)
            except KeyboardInterrupt:
                raise
            except Exception as e:
                # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ is_processing í”Œë˜ê·¸ë¥¼ í•´ì œí•´ì•¼ í•¨
                main_logger.error(f"ì…ë ¥ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                # self.is_processing = False # finally ë¸”ë¡ì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ ì—¬ê¸°ì„œ í•„ìš” ì—†ìŒ
            finally:
                # ì–´ë–¤ ê²½ìš°ë“  ì²˜ë¦¬ ì™„ë£Œ í›„ í”Œë˜ê·¸ í•´ì œ
                self.is_processing = False

    def send_to_llm(self, text):
        """
        ë™ì  ì •ì²´ì„±, STM, LTM ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ë¥¼ ë©”ì¸ LLMì— ì „ì†¡í•˜ê³ ,
        ì‘ë‹µ í›„ STM ì €ì¥ ë° LTM ì²˜ë¦¬ (ìˆ˜ì •ë¨: LTM ë¬´ì¡°ê±´ ì €ì¥).
        
        LTM ì €ì¥ì€ ë³„ë„ì˜ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
        ì´ë¥¼ í†µí•´ UI ì‘ë‹µì„±ì´ í–¥ìƒë˜ë©°, LTM ì €ì¥ì´ ì™„ë£Œë˜ì§€ ì•Šì•„ë„ ì‚¬ìš©ìëŠ” ê³„ì†í•´ì„œ
        ì–´ì‹œìŠ¤í„´íŠ¸ì™€ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """

        stm_context = "\n".join(self.short_term_memory) if self.short_term_memory else "ìµœê·¼ ëŒ€í™” ì—†ìŒ."
        stm_logger.debug(f"ì‚¬ìš©ë  STM ì»¨í…ìŠ¤íŠ¸:\n{stm_context}")

        ltm_context = "ê´€ë ¨ëœ ì¥ê¸° ê¸°ì–µ ì—†ìŒ."
        try:
            ltm_search_query = text
            ltm_search_response = self.long_term_memory.search(
                query=ltm_search_query,
                user_id=config.MEMORY_USER_ID,
                limit=3
            )
            memories_found = []
            if isinstance(ltm_search_response, list):
                memories_found = ltm_search_response

            if memories_found:
                ltm_context_lines = []
                for mem in memories_found:
                    if isinstance(mem, dict):
                        memory_text = mem.get('memory', 'ë‚´ìš© ì—†ìŒ')
                        score = mem.get('score')
                        if score is None:
                            ltm_context_lines.append(f"- {memory_text} (ê´€ë ¨ë„: N/A)")
                        else:
                            try:
                                ltm_context_lines.append(f"- {memory_text} (ê´€ë ¨ë„: {float(score):.2f})")
                            except (ValueError, TypeError):
                                ltm_context_lines.append(f"- {memory_text} (ê´€ë ¨ë„: {score})")
                if ltm_context_lines:
                    ltm_context = "\n".join(ltm_context_lines)

            ltm_logger.debug(f"ê²€ìƒ‰ëœ LTM ì»¨í…ìŠ¤íŠ¸:\n{ltm_context}")

        except Exception as e:
            ltm_logger.error(f"LTM ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            ltm_context = "ì¥ê¸° ê¸°ì–µ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ."

        try:
            dynamic_identity_context = get_astra_siro_identity_context()
            llm_logger.debug(f"ì‚¬ìš©ë  ë™ì  ì •ì²´ì„± ì»¨í…ìŠ¤íŠ¸:\n{dynamic_identity_context[:300]}...")
        except Exception as e:
            llm_logger.error(f"ë™ì  ì •ì²´ì„± ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            dynamic_identity_context = "ì˜¤ë¥˜: ì •ì²´ì„± ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


        try:
            prompt_with_context = MAIN_PROMPT_TEMPLATE.format(
                identity_context=dynamic_identity_context,
                short_term_memory=stm_context,
                long_term_memory=ltm_context,
                user_input=text
            )
            llm_logger.debug(f"ë©”ì¸ LLMì— ì „ì†¡ë  ìµœì¢… í”„ë¡¬í”„íŠ¸:\n{prompt_with_context}")
        except KeyError as e:
            llm_logger.error(f"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í¬ë§·íŒ… ì˜¤ë¥˜: ëˆ„ë½ëœ í‚¤ - {e}")
            print(f"\nâŒ ì˜¤ë¥˜: í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        except Exception as e:
            llm_logger.error(f"í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            print(f"\nâŒ ì˜¤ë¥˜: í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        payload = {
            "model": self.model,
            "prompt": prompt_with_context,
            "stream": True,
            "options": {
                "num_gpu": config.NUM_GPU,
                "temperature": self.temperature,
            }
        }
        headers = {'Content-Type': 'application/json'}
        full_response = ""

        try:
            print("\nğŸ¤– ì•„ìŠ¤íŠ¸ë¼ ì‹œë¡œ ì‘ë‹µ:")
            response = requests.post(
                self.ollama_url, json=payload, headers=headers, stream=True,
                timeout=config.REQUEST_TIMEOUT * 6
            )
            response.raise_for_status()

            for line in response.iter_lines():
                if line:
                    decoded_line = line.decode('utf-8')
                    try:
                        json_chunk = json.loads(decoded_line)
                        response_part = json_chunk.get('response', '')
                        print(response_part, end='', flush=True)
                        full_response += response_part
                        if json_chunk.get('done', False):
                            break
                    except json.JSONDecodeError:
                        llm_logger.warning(f"ì‘ë‹µ ìŠ¤íŠ¸ë¦¼ JSON ë””ì½”ë”© ì˜¤ë¥˜ (ë¬´ì‹œ): {decoded_line}")
                    except Exception as e:
                        llm_logger.error(f"ì‘ë‹µ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)

            print("\n")

            if text and full_response.strip():
                interaction_to_save = f"ì‚¬ìš©ì: {text}\nì•„ìŠ¤íŠ¸ë¼ ì‹œë¡œ: {full_response}"
                self.short_term_memory.append(interaction_to_save)
                stm_logger.info("í˜„ì¬ ëŒ€í™”ë¥¼ STMì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
                # ë¹„ë™ê¸°ì ìœ¼ë¡œ LTMì— ì €ì¥í•˜ë„ë¡ ìˆ˜ì •
                ltm_save_thread = threading.Thread(
                    target=self.save_to_ltm,
                    args=(interaction_to_save,),
                    daemon=True
                )
                ltm_save_thread.start()
                ltm_logger.info(f"LTM ì €ì¥ì„ ìœ„í•œ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘ë¨ (ID: {ltm_save_thread.ident})")

        except requests.exceptions.Timeout:
            llm_logger.error(f"Ollama API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼ ({self.ollama_url})")
            print(f"\nâŒ ì˜¤ë¥˜: LLM ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except requests.exceptions.RequestException as e:
            llm_logger.error(f"Ollama API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            print(f"\nâŒ ì˜¤ë¥˜: LLM ì„œë²„({self.ollama_url}) ì‘ë‹µì„ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ ({e}).")
        except Exception as e:
            llm_logger.error(f"LLM ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            print(f"\nâŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")



    def run_interactive_session(self):
        """ëŒ€í™”í˜• ìŒì„± ë˜ëŠ” í…ìŠ¤íŠ¸ ì„¸ì…˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        mode = "ìŒì„±" if REALTIME_STT_AVAILABLE else "í…ìŠ¤íŠ¸"
        print(f"\nğŸš€ {self.model} ëª¨ë¸ê³¼ ë™ì  ì •ì²´ì„±ì„ ì‚¬ìš©í•˜ëŠ” ì•„ìŠ¤íŠ¸ë¼ ì‹œë¡œ ({mode} ì…ë ¥ ëª¨ë“œ) ì–´ì‹œìŠ¤í„´íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("   (LTM ì¤‘ìš”ë„ í‰ê°€ ì œê±°ë¨, ëª¨ë“  ëŒ€í™” ì €ì¥)")
        print("ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ê±°ë‚˜ ë¹ˆ ì¤„ì—ì„œ Enterë¥¼ ëˆ„ë¥´ì„¸ìš” (í…ìŠ¤íŠ¸ ëª¨ë“œ).")
        print(f"ê° ëª¨ë“ˆë³„ ë¡œê·¸ëŠ” '{LOG_DIR}' ë””ë ‰í„°ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤.")
        
        try:
            while True:
                self.process_voice_input()
                time.sleep(0.1)
        except KeyboardInterrupt:
            print("\n\nCtrl+C ë˜ëŠ” ì…ë ¥ ì¢…ë£Œ ê°ì§€ë¨. ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...")
        finally:
            if hasattr(self, 'recorder') and hasattr(self.recorder, 'shutdown') and callable(self.recorder.shutdown):
                try:
                    self.recorder.shutdown()
                    stt_logger.info("STT ë ˆì½”ë”ê°€ ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    stt_logger.error(f"STT ë ˆì½”ë” ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ì–´ì‹œìŠ¤í„´íŠ¸ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")

def parse_arguments():
    """ëª…ë ¹ì¤„ ì¸ìë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    parser = argparse.ArgumentParser(description="ìŒì„±/í…ìŠ¤íŠ¸ LLM ì–´ì‹œìŠ¤í„´íŠ¸ (ë™ì  ì •ì²´ì„± ë° LTM ì¤‘ìš”ë„ í‰ê°€ ì œê±°)")
    parser.add_argument("--host", type=str, default=config.OLLAMA_HOST, help=f"ë©”ì¸ Ollama ì„œë²„ IP/í˜¸ìŠ¤íŠ¸ëª… (ê¸°ë³¸ê°’: {config.OLLAMA_HOST} from config.py)")
    parser.add_argument("--model", type=str, default=config.DEFAULT_MODEL, help=f"ì‚¬ìš©í•  ë©”ì¸ LLM ëª¨ë¸ (ê¸°ë³¸ê°’: {config.DEFAULT_MODEL} from config.py)")
    parser.add_argument("--stt", type=str, default=config.STT_MODEL, help=f"STT ëª¨ë¸ í¬ê¸° (ê¸°ë³¸ê°’: {config.STT_MODEL} from config.py) (RealtimeSTT í•„ìš”)")
    parser.add_argument("--temp", type=float, default=config.TEMPERATURE, help=f"LLM ì˜¨ë„ (ê¸°ë³¸ê°’: {config.TEMPERATURE} from config.py)")
    parser.add_argument("--cpu", action="store_true", default=not config.USE_CUDA, help=f"CUDA(GPU) ëŒ€ì‹  CPU ì‚¬ìš© (STTìš©, ê¸°ë³¸ê°’: {'CPU' if not config.USE_CUDA else 'GPU'} from config.py, RealtimeSTT í•„ìš”)")
    parser.add_argument("--debug", action="store_true", default=config.DEBUG_MODE, help=f"ìì„¸í•œ ë¡œê¹… í™œì„±í™” (DEBUG ë ˆë²¨, ê¸°ë³¸ê°’: {config.DEBUG_MODE} from config.py)")
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_arguments()

    if args.debug:
        log_level = logging.DEBUG
        main_logger.info("ëª…ë ¹ì¤„ ì¸ìë¡œ ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”ë¨ (--debug)")
    else:
        log_level = getattr(logging, config.LOG_LEVEL.upper(), logging.INFO)

    # ëª¨ë“  ë¡œê±°ì˜ ë ˆë²¨ ì„¤ì •
    loggers = [main_logger, stt_logger, ltm_logger, stm_logger, llm_logger]
    for logger in loggers:
        logger.setLevel(log_level)
        for handler in logger.handlers:
            handler.setLevel(log_level)
    main_logger.info(f"ëª¨ë“  ë¡œê±° ë ˆë²¨ ì„¤ì •ë¨: {logging.getLevelName(log_level)}")


    try:
        assistant = VoiceLLMAssistant(
            ollama_host=args.host, model=args.model, temperature=args.temp,
            stt_model=args.stt, use_cuda=not args.cpu
        )
        assistant.run_interactive_session()
    except ConnectionError as e:
        main_logger.critical(f"ì¹˜ëª…ì  ì—°ê²° ì˜¤ë¥˜: {e}")
        print(f"\nâŒ ì¹˜ëª…ì  ì—°ê²° ì˜¤ë¥˜: {e}")
        print(f"   Ollama ì„œë²„ ({args.host}:{config.OLLAMA_PORT} ë˜ëŠ” {config.MEM0_OLLAMA_BASE_URL})ê°€ ì‹¤í–‰ ì¤‘ì´ê³  ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í™•ì¸í•˜ì„¸ìš”.")
    except RuntimeError as e:
        main_logger.critical(f"ì¹˜ëª…ì  ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
        print(f"\nâŒ ì¹˜ëª…ì  ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
        print("   ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ê³  í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
    except ImportError as e:
        main_logger.critical(f"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì˜¤ë¥˜: {e}")
        print(f"\nâŒ ì¹˜ëª…ì  ì˜¤ë¥˜: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ ({e}).")
        print("   'pip install requests mem0-py' ë˜ëŠ” 'pip install RealtimeSTT' ë“±ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        main_logger.critical(f"ì–´ì‹œìŠ¤í„´íŠ¸ ì‹œì‘ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\në¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì²´í¬ë¦¬ìŠ¤íŠ¸:")
        print(f"1. ë©”ì¸ Ollama ì„œë²„ ({args.host}:{config.OLLAMA_PORT}) ì‹¤í–‰ ë° ì ‘ê·¼ ê°€ëŠ¥?")
        print(f"2. ë©”ëª¨ë¦¬ Ollama ì„œë²„ ({config.MEM0_OLLAMA_BASE_URL}) ì‹¤í–‰ ë° ì ‘ê·¼ ê°€ëŠ¥?")
        print(f"3. ChromaDB ê²½ë¡œ ('{config.CHROMA_PATH}') ì ‘ê·¼/ì“°ê¸° ê¶Œí•œ?")
        print("4. í•„ìš”í•œ Python íŒ¨í‚¤ì§€ (requests, mem0-py) ì„¤ì¹˜ í™•ì¸?")
        if REALTIME_STT_AVAILABLE:
            print("   - ìŒì„± ì…ë ¥ ì‚¬ìš© ì‹œ: RealtimeSTT íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸?")
            print("5. ì‘ë™í•˜ëŠ” ë§ˆì´í¬ ì—°ê²°? (ìŒì„± ëª¨ë“œ)")
        else:
            print("5. (RealtimeSTT ì—†ìŒ - í…ìŠ¤íŠ¸ ëª¨ë“œ)")
        print(f"6. ì§€ì •ëœ ëª¨ë¸({args.model}, {config.MEM0_LLM_MODEL}, {config.MEM0_EMBEDDING_MODEL})ì´ ê° Ollama ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œê°€ìš”?")
        print(f"7. config.py, system_prompts.py íŒŒì¼ì´ OllamaChatTest.pyì™€ ê°™ì€ í´ë”ì— ìˆë‚˜ìš”?")